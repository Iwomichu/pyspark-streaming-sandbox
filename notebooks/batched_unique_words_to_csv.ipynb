{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e371b31-f1be-4f6c-a71f-d8509e92b138",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import explode, split, col, upper, filter, udf, from_json, base64, decode, to_json, struct, encode\n",
    "from pyspark.sql.types import ArrayType, MapType, StringType, TimestampType, IntegerType, StructType, StructField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d5b99d5-7ccf-4059-8abf-d2124acda87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0\").master(\"local\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b3b33d8-3e64-4e84-8985-4f081bf0a39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_letters(s):\n",
    "    regex = re.compile('[^a-zA-Z]')\n",
    "    return regex.sub('', s)\n",
    "\n",
    "\n",
    "remove_non_letters_udf = udf(remove_non_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59dc085-3e3b-46a4-9f8f-9bb8b765bafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-----+\n",
      "|remove_non_letters(col)|count|\n",
      "+-----------------------+-----+\n",
      "+-----------------------+-----+\n",
      "\n",
      "+-----------------------+-----+\n",
      "|remove_non_letters(col)|count|\n",
      "+-----------------------+-----+\n",
      "|                  VISIT|   20|\n",
      "|               WHATEVER|   20|\n",
      "|                  ALLOW|   19|\n",
      "|               PRESSURE|   19|\n",
      "|               SOUTHERN|   19|\n",
      "|             PRODUCTION|   19|\n",
      "|                 FIGURE|   19|\n",
      "|                    MAN|   18|\n",
      "|               ELECTION|   18|\n",
      "|               BUSINESS|   18|\n",
      "|                     TV|   18|\n",
      "|              RECOGNIZE|   17|\n",
      "|               POLITICS|   17|\n",
      "|                  PEACE|   17|\n",
      "|                   TYPE|   17|\n",
      "|                 GARDEN|   17|\n",
      "|                  TOTAL|   17|\n",
      "|                   STOP|   17|\n",
      "|                  ENJOY|   17|\n",
      "|                TEACHER|   17|\n",
      "+-----------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"poster_id\", IntegerType()),\n",
    "    StructField(\"timestamp\", TimestampType()),\n",
    "    StructField(\"body\", StringType()),\n",
    "])\n",
    "\n",
    "\n",
    "KAFKA_HOST = \"kafka\"\n",
    "KAFKA_PORT = 9092\n",
    "KAFKA_TOPIC = \"posts\"\n",
    "RESULTS_DIRECTORY_PATH = \"/home/jovyan/results\"\n",
    "\n",
    "\n",
    "def unique_words(df, batch_id):\n",
    "    posts = df.withColumn(\"json\", from_json(decode(col(\"value\"), \"utf-8\"), schema)).select(\"json.*\")\n",
    "    result = posts\\\n",
    "    .distinct()\\\n",
    "    .select(explode(split(upper(col(\"body\")), \"\\s+\")))\\\n",
    "    .select(remove_non_letters_udf(\"col\"))\\\n",
    "    .groupby(\"remove_non_letters(col)\")\\\n",
    "    .count().sort(col(\"count\").desc())\n",
    "    result.show()\n",
    "    timestamp = datetime.now(tz=timezone.utc).replace(microsecond=0).isoformat()\n",
    "    result.write.csv(f\"{RESULTS_DIRECTORY_PATH}/{timestamp}\")\n",
    "    \n",
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", f\"{KAFKA_HOST}:{KAFKA_PORT}\") \\\n",
    "  .option(\"subscribe\", KAFKA_TOPIC) \\\n",
    "  .load() \n",
    "query = df.writeStream \\\n",
    "    .foreachBatch(unique_words) \\\n",
    "    .trigger(processingTime=\"1 minutes\") \\\n",
    "    .start().awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fba20ae-5b5b-4532-a1a7-5ced945e37dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
